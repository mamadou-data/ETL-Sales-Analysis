# ğŸ“Š Pipeline ETL - Analyse des Ventes avec API Kaggle, PostgreSQL & Power BI

## ğŸš€ Introduction
Ce projet met en place un **pipeline ETL automatisÃ©** permettant d'extraire, transformer et charger des donnÃ©es de ventes issues de **l'API Kaggle** dans une base **PostgreSQL**, puis d'analyser ces donnÃ©es Ã  l'aide de **Power BI**.

## ğŸ›  Technologies utilisÃ©es
- **Python** : Pandas, Requests, SQLAlchemy
- **API Kaggle** : Extraction automatisÃ©e des donnÃ©es
- **PostgreSQL** : Stockage des donnÃ©es et requÃªtes SQL
- **Power BI** : Visualisation et analyse des ventes

---

## âš™ï¸ Architecture du pipeline ETL

ğŸ“ **Ã‰tapes du pipeline :**
```
1ï¸âƒ£ Extraction : RÃ©cupÃ©ration des donnÃ©es via lâ€™API Kaggle

2ï¸âƒ£ Transformation : Nettoyage des valeurs manquantes, encodage et typage des donnÃ©es

3ï¸âƒ£ Chargement : Stockage optimisÃ© dans **PostgreSQL via "copy"

4ï¸âƒ£ Visualisation : Connexion Ã  Power BI** pour l'analyse interactive
```
---
## ğŸ”¹ Ã‰tape 1 : Extraction des donnÃ©es via l'API Kaggle
Nous utilisons l'API Kaggle pour rÃ©cupÃ©rer les donnÃ©es du dataset **Retail Sales Data**.

### ğŸ“Œ Installation et configuration de Kaggle API
1. **CrÃ©er une clÃ© API Kaggle** :
   - Se rendre sur [Kaggle](https://www.kaggle.com/), aller dans **Account Settings** et tÃ©lÃ©charger `kaggle.json`.
   - Placer `kaggle.json` dans le dossier `~/.kaggle/` (ou `%USERPROFILE%/.kaggle/` sous Windows).

2. **Installer la bibliothÃ¨que Kaggle** :
```bash
pip install kaggle
```

3. **TÃ©lÃ©charger les donnÃ©es depuis l'API** :
```python
import kaggle
```
# TÃ©lÃ©charger les donnÃ©es Kaggle
```
dataset = "noir1112/retail-sales-data"
kaggle.api.dataset_download_files(dataset, path="data/", unzip=True)
print("âœ… DonnÃ©es tÃ©lÃ©chargÃ©es avec succÃ¨s !")
```
---

## ğŸ”¹ Ã‰tape 2 : Transformation et nettoyage des donnÃ©es avec Pandas
Nous utilisons **Pandas** pour :
- **Supprimer les colonnes inutiles**
- **GÃ©rer les valeurs manquantes**
- **Convertir les formats des donnÃ©es**

```python
import pandas as pd
```
# Charger les donnÃ©es
file_path = "data/sales_100k.csv"
df = pd.read_csv(file_path, encoding='utf-8')

# Nettoyage des donnÃ©es
```
df.drop(columns=['Unnamed: 0', 'Sales_ID'], inplace=True, errors='ignore')
df.fillna({
    'Sales_Amount': df['Sales_Amount'].mean(),
    'Discount': 0,
    'Customer_Age': df['Customer_Age'].median(),
    'Customer_Gender': "Unknown"
}, inplace=True)
```
# Conversion des types
```
df['Date_of_Sale'] = pd.to_datetime(df['Date_of_Sale'], errors='coerce')
df['Customer_Age'] = df['Customer_Age'].astype('Int64')
```
# Sauvegarder en CSV pour PostgreSQL
```
file_path_cleaned = "data/ventes_clean.csv"
df.to_csv(file_path_cleaned, index=False, encoding='utf-8')
print("âœ… DonnÃ©es nettoyÃ©es et sauvegardÃ©es !")
```
---

## ğŸ”¹ Ã‰tape 3 : Chargement des donnÃ©es dans PostgreSQL
Nous utilisons PostgreSQL pour **stocker les donnÃ©es proprement et optimiser les requÃªtes**.

### ğŸ“Œ CrÃ©ation de la base et de la table
Dans **pgAdmin** ou **psql**, exÃ©cuter :
```sql
CREATE DATABASE ventes;
\c ventes;

CREATE TABLE ventes_data (
    Product_Category TEXT,
    Sales_Amount FLOAT,
    Discount FLOAT,
    Sales_Region TEXT,
    Date_of_Sale DATE,
    Customer_Age INTEGER,
    Customer_Gender TEXT,
    Sales_Representative TEXT
);
```

### ğŸ“Œ Importer les donnÃ©es dans PostgreSQL avec `\copy`
Dans **psql**, exÃ©cuter :
```sql
\copy ventes_data FROM 'data/ventes_clean.csv' WITH CSV HEADER ENCODING 'UTF8';
SELECT COUNT(*) FROM ventes_data;
```
âœ… **Les donnÃ©es sont maintenant chargÃ©es dans PostgreSQL !**

---

## ğŸ”¹ Ã‰tape 4 : Connexion et visualisation dans Power BI
### ğŸ“Œ Connexion PostgreSQL â†’ Power BI
1. **Ouvrir Power BI**
2. **Aller dans "Obtenir des donnÃ©es" â†’ "Base de donnÃ©es PostgreSQL"**
3. **Entrer les informations :**
   - **Serveur** : `localhost`
   - **Base de donnÃ©es** : `ventes`
   - **Authentification** : `postgres` / `ton_mot_de_passe`
4. **SÃ©lectionner la table `ventes_data` et charger les donnÃ©es**

### ğŸ“Œ CrÃ©ation d'un premier dashboard
1. **Ajouter un graphique en barres**
2. **Mettre "Sales_Region" en axe X et "Sales_Amount" en axe Y**
3. **Ajouter des filtres interactifs pour explorer les donnÃ©es**

âœ… **Tu as maintenant un dashboard interactif connectÃ© Ã  PostgreSQL !** ğŸ‰

---

## ğŸ“Š RÃ©sultats et insights
### **ğŸ” Analyse des ventes par rÃ©gion :**
```sql
SELECT Sales_Region, COUNT(*) AS nb_ventes, SUM(Sales_Amount) AS total_ventes
FROM ventes_data
GROUP BY Sales_Region
ORDER BY total_ventes DESC;
```
ğŸ“Œ **Ce type dâ€™analyse permet d'identifier les meilleures performances commerciales par rÃ©gion.**

---

## ğŸ“Œ Comment exÃ©cuter ce projet ?
### 1ï¸âƒ£ Installer les dÃ©pendances
```bash
pip install pandas kaggle sqlalchemy psycopg2
```
### 2ï¸âƒ£ ExÃ©cuter le pipeline
```bash
python etl_postgresql.py
```
### 3ï¸âƒ£ VÃ©rifier les donnÃ©es dans PostgreSQL
```sql
SELECT COUNT(*) FROM ventes_data;
```
### 4ï¸âƒ£ Ouvrir Power BI et se connecter Ã  PostgreSQL

---

## ğŸ¯ Conclusion
Ce projet montre comment **automatiser un pipeline ETL** depuis **lâ€™API Kaggle** jusquâ€™Ã  **Power BI**, en passant par **PostgreSQL**. ğŸ¯

ğŸ“¢ **AmÃ©liorations possibles :**
- Automatiser l'exÃ©cution avec **Airflow**
- Ajouter une API Flask pour interroger les donnÃ©es
- DÃ©ployer un dashboard web interactif

ğŸš€ **Nâ€™hÃ©sitez pas Ã  contribuer ou poser des questions !*

